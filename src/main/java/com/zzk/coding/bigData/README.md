# 大数据和空间限制

## 1.认识布隆过滤器
### 题目
不安全网页和黑名单包含100亿个黑名单网页，每个网页的URL最多占用64B，现在想要实现一种网页过滤系统，可以根据网页的URL判断是否在黑名单上，
请设计该系统

### 要求
1.该系统允许有万分之一以下的判断失误率
2.使用的额外空间不要超过30GB

### 思路
如果把黑名单所有的URL通过数据库或哈希表保存下来，就可以对每条URL进行查询。但是每个URL有64B，数量是100亿个，所以至少需要64GB的空间，
不满足要求2，所以可以用布隆过滤器来进行判断

## 2.只用2GB内存在20亿个整数中找到出现次数最多的数  

### 题目
有一个包含20亿个全是32位整数的大文件，在其中找到出现次数最多的数

### 要求
内存限制为2GB

### 解答
   想要在很多整数中找到出现次数最多的数，通常的做法是使用哈希表对出现的每个数做词频统计，哈希表的key是某一个整数，value是这个数
出现的次数，就本题来说，一共有20亿个数，哪怕只是一个数出现了20亿次，用32位的整数也可以表示其出现的次数而不会溢出
（32位整数范围为 -2^31 ~ 2^32 -1 -2147483648 ~ 2147483647）,所以哈希表的key需要占用4B，value也是4B，那么哈希表的一条记录
（key，value）需要占用8B，当哈希表记录数为2亿个时，需要至少1.6GB的内存
   但如果20亿个数中不同的数超过2亿种，最极端的情况是20亿个数都不同，那么在哈希表中可能产生20亿条记录，这样内存会不够用，所以一次性用
哈希表统计20亿个数的办法是很大风险的
   解决办法是把包含20亿个数的大文件用哈希函数分成16个小文件，根据哈希函数的性质，同一种数不可能被哈希到不同的小文件上，同时每个小文件中
不同的数一定不会大于2亿种，假设哈希函数足够好。然后对每一个小文件用哈希表来统计其中每种数出现的次数，这样就可以得到16个小文件中各自出现次数
最多的数，以及各自统计的次数，接下来只要选择出16个小文件各自的第一名中谁出现的次数最多即可
    把一个大的集合通过哈希函数分配到多台机器中，或者分配到多少个文件里，这种技巧是处理大数据面试题时最常用的技巧之一，但是到底分配到多少机器、
分配到多少文件，在解题时一定要确定下来，可能是在与面试官沟通的过程中由面试官指定，也可能是根据具体的限制来确定，比如本题确定为16个文件，就是根据
内存限制2GB条件来确定的

## 3.40亿个非负整数中找到没出现的数

### 题目
   32位无符号整数的范围是0 ~ 4294967295，现在有一个正好包含40亿个无符号整数的文件，所以在整个范围中必然有会没有出现的数，可以使用最多1GB的内存，
怎么找到所以没出现过的数？
   进阶：内存限制为10MB，但是只用找到一个没出现过的数即可
   
### 解答
   原问题。如果用哈希表来保存出现过的数，那么如果40亿个数都不同，则哈希表的记录有40亿条，存一个32位整数需要4B，所以最差情况下需要40亿*4B=160亿字节，
大约需要16GB的空间，不符合要求
   哈希表需要占用很多空间，我们可以使用bit map的方式来表示数出现的情况。具体地说，是申请一个长度为4294967295的bit 类型的数组bitArr，bitArr上的
每个位置只可以表示0或者1状态。8个bit为1B，所以长度为4294967295的bit类型的数组占用500MB的空间。遍历40亿个无符号数，遇见所有的数时，把bitArr相应位置
的值设置为1，遍历完成后再依次遍历bitArr，所有没有出现的数都找出来了
    进阶。
1. 根据10MB的内存限制，确定统计区间的大小，就是第二次遍历时的bitArr大小
2. 利用区间计数的方式。找到那个计数不足的区间，这个区间上肯定有没出现过的数
3. 对这个区间上的数做bit map映射，再遍历bit map，找到一个没有出现的数即可

## 4.找到100亿个URL中重复的URL以及搜索词汇的top K问题
### 题目
有一个包含100亿个URL的大文件，假设每个URL占用64B，请找出其中所有重复的URL

### 补充题目
某搜索公司一天的用户搜索词汇是海量的（百亿数据量），请设计一种求出每天最热top 100 词汇的可行办法

### 解答
   原问题的解法使用解决大数据问题的一种常规方法：把大文件通过哈希函数分配到机器，或通过哈希函数把大文件拆分成小文件。一直进行这种划分，直到划分的结果满足资源限制的要求。
首先，你要向面试官询问在资源上的限制有哪些，包括内存，计算时间等要求。在明确了限制要求之后，可以将每条URL通过哈希函数分配到若干机器或者拆分成若干小文件，这里若干
由具体资源限制来计算出精确的数量
   例如，将100亿字节的大文件通过哈希函数分配到100台机器上，然后每台机器分别统计分给自己的URL是否有重复的URL，同时哈希函数的性质决定了同一条URL不可能分给不同的机器；或者
在单机上将大文件通过哈希函数拆分成1000个小文件，对每一个小文件再利用哈希表遍历，找出重复的URL；或者在分给机器或拆完文件之后，进行排序，排序后再看看是否有重复的URL出现。总之，
牢记一点，很多大数据问题都离不开分流，要么是哈希函数把大文件的内容分配给不同的机器，要么是哈希函数把大文件拆分成小文件，然后处理每一个小数量的集合
    补充问题最开始还是用哈希分流的思路来处理，把包含百亿数据量的词汇文件分流到不同的机器上，具体多少台机器由面试官规定或者由更多的限制来决定。对每一台机器来说，如果分到的数据量
依然很大，比如内存不够或者其他问题，可以再用哈希函数把每台机器的分流文件拆分成更小的文件处理。处理每一个小文件的时候，哈希表统计每种词以及词频，哈希表记录建立完成后，在遍历哈希表，
遍历哈希表的过程用大小为100的小根堆来选每一个小文件的top 100（整体未排序的top 100）.每一个小文件都有自己词频的小根堆，将小根堆里的词按照词频排序，就得到了每个小文件的排序后top 100.
然后把各个小文件排序后的top 100在进行排序，最终求出整个百亿数据量中的top 100.对于top K 的问题，除哈希函数分流和用哈希表做词频统计之外，还经常用堆结构和外排序的手段进行处理

## 5.40亿个非负整数中找到出现两次的数和所有数的中位数
### 题目
32位无符号整数的范围是0 ~ 4294967295，现在有40亿个无符号整数，可以使用最多1GB的内存，找出所有出现了两次的数

### 补充题目
可以使用最多10MB的内存，怎么找到这40亿个整数的中位数？

### 解答
   对于原问题，可以使用bit map的方式来表示数出现的情况。具体的说是，申请一个长度为4294967295 * 2的bit类型的数组bitArr，用2个位置表示一个数出现的词频，1B占用8个bit，所以长度为4294967295*2
的bit类型的数组占用1GB空间。怎么使用这个bitArr数组呢？遍历这40亿个无符号数，如果初次遇见num，就把bitArr[num*2+1]和
bitArr[num*2]设置为01，如果第二次遇见num，就把bitArr[num*2+1]和bitArr[num*2]设置为10，如果第三次遇见num，就把bitArr[num*2+1]和bitArr[num*2]设置为11.以后再遇见num，发现此时
bitArr[num*2+1]和bitArr[num*2]已经被设置为11，就不再做任何设置。遍历完成后，再依次遍历bitArr，如果发现bitArr[i*2+1]和bitArr[i*2]设置为10，那么i就是出现了两次的数
    对于补充问题，用分区间的方式处理，长度为2MB的无符号整型数组占用的空间为8MB，所以将区间的数量定位4294967295/2M=2148个。申请一个长度为2148的无符号整型数组arr[0...2147],
arr[i]表示第i区间有多少个数。然后遍历这40亿个数，将对应的进行arr[num/2M]++操作，然后累加每个区间的出现次数，如果0~K-1区间上的数的个数为19.998亿，但是当加上第K个区间上的数的
个数后就超过了20亿，因此第20亿个数是第K区间上第0.002亿个数。
    然后申请一个长度为2MB的无符号整型数组countArr[0...2M-1]，占用空间是8MB。然后遍历这40亿个数，只对第K区间的数做统计，jicountArr[numi - K * 2M]++，然后只在第K区间上找到第0.002亿个数即可。


## 6.一致性哈希算法的基本原理
### 题目
   工程师常使用服务器集群来设计和实现数据缓存，以下是常见的策略：
1.无论是添加、查询还是删除数据，都先将数据的id通过哈希函数转换成一个哈希的值，记为key
2.如果目前机器有N台，则计算key%N的值，这个值就是该数据所属的机器编号，无论是添加、删除还是查询操作，都只在这台机器上进行
    请分析这种缓存策略可能带来的问题，并提出改进的方案

